{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkhUiV/VJRHWsaHm/5jJRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gayathri-achari/Projects/blob/Stock-price-Prediction/Stock_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GRU, Conv1D, Dense, Flatten, Input, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load stock data\n",
        "data = pd.read_csv('/content/Google_test_data.csv')  # Ensure you have Google stock data\n",
        "prices = data['Close'].values\n",
        "prices = (prices - np.min(prices)) / (np.max(prices) - np.min(prices))  # Normalize\n",
        "\n",
        "# Prepare dataset\n",
        "seq_len = 7\n",
        "X, y = [], []\n",
        "for i in range(len(prices) - seq_len):\n",
        "    X.append(prices[i:i+seq_len])\n",
        "    y.append(prices[i+seq_len])\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "def build_generator():\n",
        "    inp = Input(shape=(seq_len, 1))\n",
        "    x = GRU(64, return_sequences=True)(inp)\n",
        "    x = GRU(64)(x)\n",
        "    out = Dense(seq_len, activation='relu')(x)  # Ensure positive values\n",
        "    # Reshape output to (None, seq_len, 1) which is the expected input shape for Conv1D in discriminator\n",
        "    out = Reshape((seq_len, 1))(out)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Discriminator model\n",
        "def build_discriminator():\n",
        "    inp = Input(shape=(seq_len, 1))\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(inp)\n",
        "    x = Flatten()(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "# Instantiate models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile discriminator\n",
        "# The discriminator will be trained independently on real and fake data.\n",
        "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
        "\n",
        "# GAN model\n",
        "# In the GAN, the generator's output is fed into the discriminator.\n",
        "gan_input = Input(shape=(seq_len, 1))\n",
        "generated_price_for_gan = generator(gan_input)\n",
        "discriminator.trainable = False # Freeze discriminator weights when training the generator through the GAN\n",
        "validity_of_generated = discriminator(generated_price_for_gan)\n",
        "gan = Model(gan_input, validity_of_generated)\n",
        "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
        "\n",
        "# Training\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of real data\n",
        "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
        "    real_data = X[idx] # Shape is (batch_size, seq_len)\n",
        "\n",
        "    # Ensure real_data has the correct shape (batch_size, seq_len, 1) for the models\n",
        "    # We need to add the channel dimension for both real and fake data.\n",
        "    real_data = real_data.reshape(batch_size, seq_len, 1)\n",
        "\n",
        "    # Generate fake data from the generator\n",
        "    # generator.predict expects input shape (batch_size, seq_len, 1)\n",
        "    # The output of generator.predict will have shape (batch_size, seq_len, 1) due to the Reshape layer\n",
        "    fake_data = generator.predict(real_data)\n",
        "\n",
        "    # Labels for the discriminator\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator\n",
        "    # discriminator.train_on_batch expects input shape (batch_size, seq_len, 1)\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator through the GAN\n",
        "    # We want the generator to fool the discriminator, so we use real_labels\n",
        "    # gan.train_on_batch expects input shape (batch_size, seq_len, 1)\n",
        "    g_loss = gan.train_on_batch(real_data, real_labels)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
        "\n",
        "# Predict next-day stock price\n",
        "# Ensure the input to predict is also (1, seq_len, 1)\n",
        "# X[-1] has shape (seq_len,). Reshape to (1, seq_len, 1)\n",
        "input_for_prediction = X[-1].reshape(1, seq_len, 1)\n",
        "next_day_prediction = generator.predict(input_for_prediction)\n",
        "\n",
        "# The output shape of generator.predict is (1, seq_len, 1).\n",
        "# Depending on what you want to predict (e.g., the next single value or the next sequence),\n",
        "# you'll need to adjust how you extract the result.\n",
        "# If predicting the next single price based on the last sequence:\n",
        "# The generator output is a sequence of 'seq_len' values. If the goal is to predict the single next value,\n",
        "# the generator architecture might need adjustment (e.g., a Dense(1) layer at the end if not predicting a sequence).\n",
        "# Assuming the intention was to predict the *next sequence* of length seq_len starting from the last data point:\n",
        "print(f\"Predicted Next-Day Prices Sequence: {next_day_prediction[0]}\")\n",
        "# If you truly meant the single next day price, you might need to reconsider the generator's output layer.\n",
        "# For this code as written, it predicts a sequence of length seq_len."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymvlQfvb4iwv",
        "outputId": "40aaf78c-9a18-4052-e640-7458e4a429a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | D Loss: 0.6930 | G Loss: 0.6923\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 2/15 | D Loss: 0.6931 | G Loss: 0.6921\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 3/15 | D Loss: 0.6934 | G Loss: 0.6920\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Epoch 4/15 | D Loss: 0.6934 | G Loss: 0.6918\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Epoch 5/15 | D Loss: 0.6934 | G Loss: 0.6917\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Epoch 6/15 | D Loss: 0.6936 | G Loss: 0.6915\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 7/15 | D Loss: 0.6937 | G Loss: 0.6913\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Epoch 8/15 | D Loss: 0.6938 | G Loss: 0.6911\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Epoch 9/15 | D Loss: 0.6939 | G Loss: 0.6909\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Epoch 10/15 | D Loss: 0.6941 | G Loss: 0.6907\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 11/15 | D Loss: 0.6942 | G Loss: 0.6904\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Epoch 12/15 | D Loss: 0.6943 | G Loss: 0.6902\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 13/15 | D Loss: 0.6944 | G Loss: 0.6900\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Epoch 14/15 | D Loss: 0.6945 | G Loss: 0.6898\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 15/15 | D Loss: 0.6946 | G Loss: 0.6895\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step\n",
            "Predicted Next-Day Prices Sequence: [[0.24816531]\n",
            " [0.17186044]\n",
            " [0.05004271]\n",
            " [0.12514707]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GRU, Conv1D, Dense, Flatten, Input, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load stock data\n",
        "data = pd.read_csv('/content/Google_test_data.csv')  # Ensure you have Google stock data\n",
        "prices = data['Close'].values\n",
        "prices = (prices - np.min(prices)) / (np.max(prices) - np.min(prices))  # Normalize\n",
        "\n",
        "# Prepare dataset\n",
        "seq_len = 7\n",
        "X, y = [], []\n",
        "for i in range(len(prices) - seq_len):\n",
        "    X.append(prices[i:i+seq_len])\n",
        "    y.append(prices[i+seq_len])\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "def build_generator():\n",
        "    inp = Input(shape=(seq_len, 1))\n",
        "    x = GRU(64, return_sequences=True)(inp)\n",
        "    x = GRU(64)(x)\n",
        "    out = Dense(seq_len, activation='relu')(x)  # Ensure positive values\n",
        "    # Reshape output to (None, seq_len, 1) which is the expected input shape for Conv1D in discriminator\n",
        "    out = Reshape((seq_len, 1))(out)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Discriminator model\n",
        "def build_discriminator():\n",
        "    inp = Input(shape=(seq_len, 1))\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu')(inp)\n",
        "    x = Flatten()(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "# Instantiate models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile discriminator\n",
        "# The discriminator will be trained independently on real and fake data.\n",
        "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
        "\n",
        "# GAN model\n",
        "# In the GAN, the generator's output is fed into the discriminator.\n",
        "gan_input = Input(shape=(seq_len, 1))\n",
        "generated_price_for_gan = generator(gan_input)\n",
        "discriminator.trainable = False # Freeze discriminator weights when training the generator through the GAN\n",
        "validity_of_generated = discriminator(generated_price_for_gan)\n",
        "gan = Model(gan_input, validity_of_generated)\n",
        "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
        "\n",
        "# Training\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of real data\n",
        "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
        "    real_data = X[idx] # Shape is (batch_size, seq_len)\n",
        "\n",
        "    # Ensure real_data has the correct shape (batch_size, seq_len, 1) for the models\n",
        "    # We need to add the channel dimension for both real and fake data.\n",
        "    real_data = real_data.reshape(batch_size, seq_len, 1)\n",
        "\n",
        "    # Generate fake data from the generator\n",
        "    # generator.predict expects input shape (batch_size, seq_len, 1)\n",
        "    # The output of generator.predict will have shape (batch_size, seq_len, 1) due to the Reshape layer\n",
        "    fake_data = generator.predict(real_data)\n",
        "\n",
        "    # Labels for the discriminator\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the discriminator\n",
        "    # discriminator.train_on_batch expects input shape (batch_size, seq_len, 1)\n",
        "    d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator through the GAN\n",
        "    # We want the generator to fool the discriminator, so we use real_labels\n",
        "    # gan.train_on_batch expects input shape (batch_size, seq_len, 1)\n",
        "    g_loss = gan.train_on_batch(real_data, real_labels)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
        "\n",
        "# Predict next-day stock price\n",
        "# Ensure the input to predict is also (1, seq_len, 1)\n",
        "# X[-1] has shape (seq_len,). Reshape to (1, seq_len, 1)\n",
        "input_for_prediction = X[-1].reshape(1, seq_len, 1)\n",
        "next_day_prediction = generator.predict(input_for_prediction)\n",
        "\n",
        "# The output shape of generator.predict is (1, seq_len, 1).\n",
        "# Depending on what you want to predict (e.g., the next single value or the next sequence),\n",
        "# you'll need to adjust how you extract the result.\n",
        "# If predicting the next single price based on the last sequence:\n",
        "# The generator output is a sequence of 'seq_len' values. If the goal is to predict the single next value,\n",
        "# the generator architecture might need adjustment (e.g., a Dense(1) layer at the end if not predicting a sequence).\n",
        "# Assuming the intention was to predict the *next sequence* of length seq_len starting from the last data point:\n",
        "\n",
        "# If you truly meant the single next day price, you might need to reconsider the generator's output layer.\n",
        "# For this code as written, it predicts a sequence of length seq_len.\n",
        "# Round values and format output\n",
        "# Convert array to list format with better readability\n",
        "# Convert array to list format with better readability\n",
        "formatted_prediction = next_day_prediction[0].tolist()\n",
        "\n",
        "# Print in a compact format\n",
        "print(\"Predicted Next-Day Prices Sequence:\", formatted_prediction)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9887c673-8c56-4f96-c1e3-acae21d9123f",
        "id": "47Jg4rQm7JTL"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | D Loss: 0.6746 | G Loss: 0.6911\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 2/15 | D Loss: 0.6792 | G Loss: 0.6908\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 3/15 | D Loss: 0.6800 | G Loss: 0.6904\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 4/15 | D Loss: 0.6806 | G Loss: 0.6900\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 5/15 | D Loss: 0.6811 | G Loss: 0.6897\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Epoch 6/15 | D Loss: 0.6815 | G Loss: 0.6894\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 7/15 | D Loss: 0.6817 | G Loss: 0.6890\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Epoch 8/15 | D Loss: 0.6818 | G Loss: 0.6885\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 9/15 | D Loss: 0.6820 | G Loss: 0.6881\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Epoch 10/15 | D Loss: 0.6823 | G Loss: 0.6877\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Epoch 11/15 | D Loss: 0.6825 | G Loss: 0.6873\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Epoch 12/15 | D Loss: 0.6827 | G Loss: 0.6869\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 13/15 | D Loss: 0.6830 | G Loss: 0.6865\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Epoch 14/15 | D Loss: 0.6832 | G Loss: 0.6860\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Epoch 15/15 | D Loss: 0.6834 | G Loss: 0.6855\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step\n",
            "Predicted Next-Day Prices Sequence: [[0.0], [0.21461237967014313], [0.2090194672346115], [0.0], [0.0], [0.21650683879852295], [0.0]]\n"
          ]
        }
      ]
    }
  ]
}